{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®Œæˆä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºhugging faceå®˜ç½‘æ€»æ˜¯è¿æ¥ä¸ä¸Šï¼Œå› æ­¤åœ¨ä¸‹è½½æ¨¡å‹æ—¶ä½¿ç”¨å›½å†…çš„é•œåƒç½‘ç«™ï¼Œæˆ–æ˜¯å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°å†è¿›è¡ŒåŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # è®¾ç½®é•œåƒåœ°å€ï¼ˆä»£ç çº§è¦†ç›–ï¼‰\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# è®¾ç½®æ–‡ä»¶å­˜æ”¾è·¯å¾„\n",
    "data_path = \".\\data\\GLUE\\MRPC\"\n",
    "model_path = \".\\models\\\\bert-base-uncased\"\n",
    "save_path = \".\\output\"\n",
    "checkpoint = \"bert-base-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizeré¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨AutoTokenizerç±»ä¸‹è½½åˆ†è¯å™¨å¹¶å®ä¾‹åŒ–ï¼Œæˆ‘æ˜¯æŠŠtokenizerå’Œmodelçš„configä¸‹è½½ä¸‹æ¥æ”¾åˆ°æœ¬åœ°ç„¶åè½½å…¥çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Anaconda_enviroments\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#return_tensors=\"pt\"è¡¨ç¤ºè¿”å›Pytorchå¼ é‡ã€‚æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ä¹‹åå¿…é¡»å†è½¬æ¢æˆå¼ é‡tensorsæ‰èƒ½è¾“å…¥æ¨¡å‹ã€‚\n",
    "#padding=Trueè¡¨ç¤ºå¡«å……è¾“å…¥åºåˆ—åˆ°æœ€å¤§é•¿åº¦ï¼Œtruncation=Trueè¡¨ç¤ºè¿‡é•¿åºåˆ—è¢«æˆªæ–­\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é€‰æ‹©æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨AutoModelç±»ä¸‹è½½æ¨¡å‹å¹¶å®ä¾‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å°å¹¶æŸ¥çœ‹æ¨¡å‹ç»“æ„\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡å¿…é¡»è½½å…¥å¯¹åº”çš„model headï¼Œå› æ­¤ä¸ä½¿ç”¨AutoModelç±»ï¼Œè€Œæ˜¯ä½¿ç”¨AutoModelForSequenceClassificationç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at .\\models\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 0.0773, -0.2972],\n",
      "        [ 0.0211, -0.3956]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ç”±äºåªæœ‰ä¸¤ä¸ªå¥å­å’Œä¸¤ä¸ªæ ‡ç­¾ï¼Œå› æ­¤å¾—åˆ°äº†2*2å½¢çŠ¶çš„ç»“æœ\n",
    "print(outputs.logits.shape)\n",
    "\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨pytorchä¸­ï¼Œè°ƒç”¨nn.crossentropyloss()ä¹‹å‰ä¸€å®šä¸è¦å†è°ƒç”¨nn.softmax()å‡½æ•°äº†ï¼Œå› ä¸ºnn.crossentropyloss()é‡Œå·²ç»åŒ…å«äº†softmaxçš„è®¡ç®—ï¼Œå®ƒçš„è®¡ç®—èåˆäº†logsoftmaxå’Œnulllossä¸¤éƒ¨åˆ†:\\\n",
    "\\\n",
    "nn.softmax() = nn.logsoftmax() + nn.nllloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5925, 0.4075],\n",
      "        [0.6027, 0.3973]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "predictions = F.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)  # è¾“å‡ºä¸ºæ¦‚ç‡åˆ†æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹åˆ†ç±»æ ‡ç­¾ï¼Œå¾—å‡ºæ¨¡å‹é¢„æµ‹çš„ç»“è®ºå¦‚ä¸‹ï¼š\\\n",
    "ç¬¬ä¸€å¥ï¼šNEGATIVE - 0.040195ï¼ŒPOSITIVE - 0.95980\\\n",
    "ç¬¬ä¸€å¥ï¼šNEGATIVE - 0.99946ï¼ŒPOSITIVE - 0.00054418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹å…±æœ‰å‡ ä¸ªåˆ†ç±»æ ‡ç­¾(è¿™é‡Œåªæœ‰0,1ä¸¤ç±»)\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‹è½½å¹¶æ„å»ºè‡ªå·±çš„æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE benchmark(GLUEåŸºå‡†)ç”¨äºè¡¡é‡nlpæ¨¡å‹åœ¨9ä¸ªä¸åŒä»»åŠ¡(åŒ…å«åˆ†ç±»ä»»åŠ¡ã€ç›¸ä¼¼åº¦ä»»åŠ¡ã€æ¨ç†ä»»åŠ¡)ä¸­çš„æ€§èƒ½\\\n",
    "MRPCæ•°æ®é›†æ˜¯å…¶ä¸­ä¹‹ä¸€\\\n",
    "\\\n",
    "ç”±äºGLUEä¸­çš„æ•°æ®é›†å‡å¸¦æœ‰æ ‡ç­¾(æ˜¯æ ‡æ³¨æ•°æ®)ï¼Œå› æ­¤å¾®è°ƒbertæ¨¡å‹å¯ä»¥ç®—ä½œæ˜¯ç›‘ç£å¾®è°ƒsft(supervised fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3668 examples [00:00, 319575.98 examples/s]\n",
      "Generating validation split: 408 examples [00:00, 102025.64 examples/s]\n",
      "Generating test split: 1725 examples [00:00, 431101.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ä»æœ¬åœ°è¯»å–æ•°æ®é›†\n",
    "raw_datasets = load_dataset(\"parquet\", \n",
    "                            data_files={\n",
    "                                \"train\": os.path.join(data_path, \"train-00000-of-00001.parquet\"), \n",
    "                                \"validation\": os.path.join(data_path, \"validation-00000-of-00001.parquet\"),\n",
    "                                \"test\": os.path.join(data_path, \"test-00000-of-00001.parquet\")\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¾“å‡ºè®­ç»ƒé›†ä¸­çš„ç¬¬ä¸€æ¡æ•°æ®ï¼Œå±•ç¤ºæ•°æ®æ ¼å¼\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é€šè¿‡featureså±æ€§å¯ä»¥çŸ¥é“æ¯ä¸€åˆ—çš„ç±»å‹\n",
    "# ç”±äºMRPCæ•°æ®é›†æ˜¯ä¸€ä¸ªç›¸ä¼¼åº¦ä»»åŠ¡æ•°æ®é›†ï¼Œå› æ­¤labelä¸º0è¡¨ç¤ºå¥å­å¯¹æ„æ€ä¸ä¸€è‡´ï¼Œä¸º1è¡¨ç¤ºå¥å­å¯¹æ„æ€ä¸€è‡´\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿˜å¯ä»¥è½¬æˆdfå½¢å¼å¯¹æ•°æ®é›†è¿›è¡Œå¯è§†åŒ–åŠåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "validation = pd.DataFrame(raw_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He said the foodservice pie business doesn 't ...</td>\n",
       "      <td>\" The foodservice pie business does not fit ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Magnarelli said Racicot hated the Iraqi regime...</td>\n",
       "      <td>His wife said he was \" 100 percent behind Geor...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The dollar was at 116.92 yen against the yen ,...</td>\n",
       "      <td>The dollar was at 116.78 yen JPY = , virtually...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The AFL-CIO is waiting until October to decide...</td>\n",
       "      <td>The AFL-CIO announced Wednesday that it will d...</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No dates have been set for the civil or the cr...</td>\n",
       "      <td>No dates have been set for the criminal or civ...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Their contract will expire at 12 : 01 a.m. Wed...</td>\n",
       "      <td>\" It has outraged the membership , \" said Rian...</td>\n",
       "      <td>0</td>\n",
       "      <td>4023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>But plaque volume increased by 2.7 percent in ...</td>\n",
       "      <td>The volume of plaque in Pravachol patients ' a...</td>\n",
       "      <td>1</td>\n",
       "      <td>4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Today in the US , the book - kept under wraps ...</td>\n",
       "      <td>Tomorrow the book , kept under wraps by G. P. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>The S &amp; P / TSX composite rose 87.74 points on...</td>\n",
       "      <td>On the week , the Dow Jones industrial average...</td>\n",
       "      <td>0</td>\n",
       "      <td>4049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Ex-KGB agent Putin added that the Beatles were...</td>\n",
       "      <td>In Soviet times the Beatles ' music \" was cons...</td>\n",
       "      <td>1</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "0    He said the foodservice pie business doesn 't ...   \n",
       "1    Magnarelli said Racicot hated the Iraqi regime...   \n",
       "2    The dollar was at 116.92 yen against the yen ,...   \n",
       "3    The AFL-CIO is waiting until October to decide...   \n",
       "4    No dates have been set for the civil or the cr...   \n",
       "..                                                 ...   \n",
       "403  Their contract will expire at 12 : 01 a.m. Wed...   \n",
       "404  But plaque volume increased by 2.7 percent in ...   \n",
       "405  Today in the US , the book - kept under wraps ...   \n",
       "406  The S & P / TSX composite rose 87.74 points on...   \n",
       "407  Ex-KGB agent Putin added that the Beatles were...   \n",
       "\n",
       "                                             sentence2  label   idx  \n",
       "0    \" The foodservice pie business does not fit ou...      1     9  \n",
       "1    His wife said he was \" 100 percent behind Geor...      0    18  \n",
       "2    The dollar was at 116.78 yen JPY = , virtually...      0    25  \n",
       "3    The AFL-CIO announced Wednesday that it will d...      1    32  \n",
       "4    No dates have been set for the criminal or civ...      0    33  \n",
       "..                                                 ...    ...   ...  \n",
       "403  \" It has outraged the membership , \" said Rian...      0  4023  \n",
       "404  The volume of plaque in Pravachol patients ' a...      1  4028  \n",
       "405  Tomorrow the book , kept under wraps by G. P. ...      1  4040  \n",
       "406  On the week , the Dow Jones industrial average...      0  4049  \n",
       "407  In Soviet times the Beatles ' music \" was cons...      1  4053  \n",
       "\n",
       "[408 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®é›†é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸¾ä¸ªä¾‹å­æ¥çœ‹tokenizerçš„è¾“å‡º\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3668/3668 [00:00<00:00, 20522.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:00<00:00, 11168.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1725/1725 [00:00<00:00, 18445.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œåˆ†è¯å¤„ç†\n",
    "# # ä»¥ä¸‹è¿™ç§åšæ³•æ˜¯å¯ä»¥çš„ï¼Œä½†ç¼ºç‚¹æ˜¯å¤„ç†åçš„tokenized_datasetå°†ä¸å†æ˜¯datasetæ ¼å¼ï¼Œè€Œæ˜¯è¿”å›å­—å…¸\n",
    "# # ä¸€æ—¦datasetè¿‡å¤§ï¼Œå°±æ— æ³•å­˜æ”¾åœ¨å†…å­˜ä¸­ï¼Œä¼šå¯¼è‡´out of memoryå¼‚å¸¸\n",
    "# tokenized_dataset = tokenizer(\n",
    "#     raw_datasets[\"train\"][\"sentence1\"],\n",
    "#     raw_datasets[\"train\"][\"sentence2\"],\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "# )\n",
    "\n",
    "# ä¸ºäº†ä½¿æ•°æ®é›†ä¿æŒdatasetæ ¼å¼ï¼Œä½¿ç”¨æ›´çµæ´»çš„dataset.mapæ–¹æ³•\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "# åŠ¨æ€å¡«å……ï¼Œå³å°†æ¯ä¸ªæ‰¹æ¬¡çš„è¾“å…¥åºåˆ—å¡«å……åˆ°ä¸€æ ·çš„é•¿åº¦\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨Trainer APIè¿›è¡Œè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“æœªè®¾ç½®è¯„ä¼°å‡½æ•°æ—¶ï¼Œtrainerè¾“å‡ºçš„ç»“æœä¸­ä»…æœ‰training lossï¼Œå¹¶ä¸åŒ…å«è¯„ä¼°ç»“æœaccã€f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# TrainingArgumentsç±»å”¯ä¸€ä¸€ä¸ªå¿…é¡»æä¾›çš„å‚æ•°æ˜¯ä¿å­˜modelçš„è·¯å¾„\n",
    "training_args = TrainingArguments(os.path.join(save_path, checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at D:\\ProgrammingProjects\\huggingface\\models\\bert\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# å®ä¾‹åŒ–è¯¥é¢„è®­ç»ƒæ¨¡å‹åæ±‡æŠ¥ä¸€ä¸ªwarningï¼Œå› ä¸ºBERTæ²¡æœ‰åœ¨å¥å­åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)  # äºŒåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªè®­ç»ƒå™¨\n",
    "# å°†æ„å»ºçš„æ‰€æœ‰å¯¹è±¡ä¼ å…¥è¿›è¡Œæ¨¡å‹ç²¾è°ƒ\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 500/1377 [21:42<39:56,  2.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5368, 'grad_norm': 7.87819242477417, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1000/1377 [1:36:34<15:44,  2.50s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2799, 'grad_norm': 5.622527599334717, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1377/1377 [1:52:57<00:00,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6777.5719, 'train_samples_per_second': 1.624, 'train_steps_per_second': 0.203, 'train_loss': 0.3399341530616334, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3399341530616334, metrics={'train_runtime': 6777.5719, 'train_samples_per_second': 1.624, 'train_steps_per_second': 0.203, 'total_flos': 405114969714960.0, 'train_loss': 0.3399341530616334, 'epoch': 3.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åœ¨CPUä¸Šå¾®è°ƒæ¨¡å‹ç‰¹åˆ«æ…¢ï¼Œå¯ä»¥æŒ‚è½½åˆ°colabä¸Šç”¨GPUåŠ é€Ÿ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 408\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹éªŒè¯é›†é¢„å¤„ç†åçš„ç»“æ„\n",
    "tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:32<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# è·å¾—æ¨¡å‹é¢„æµ‹ç»“æœ\n",
    "# predictæ–¹æ³•è¾“å‡ºä¸€ä¸ªä¸‰å…ƒç»„(predictions:[batch_size, num_labels], label_ids, metrics:é»˜è®¤training loss)\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions:[batch_size, num_labels]ï¼Œæ¯”è¾ƒæ ‡ç­¾å’Œé¢„æµ‹ç»“æœ\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¾ç½®è¯„ä¼°å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load(\"glue\", \"mrpc\")  # æˆ–è€… load(\"glue/mrpc\")\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è®¾å®šå®Œéœ€è¦è¾“å‡ºçš„è¯„ä¼°æŒ‡æ ‡åé‡æ–°è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Anaconda_enviroments\\envs\\transformers\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at .\\models\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "# training_args = TrainingArguments(model_path, \n",
    "#                                 evaluation_strategy=\"epoch\")\n",
    "# æ³¨ï¼šTrainingArgumentså”¯ä¸€ä¸€ä¸ªå¿…é¡»ä¼ å…¥çš„å‚æ•°æ˜¯ä¿å­˜modelçš„è·¯å¾„\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    run_name=checkpoint,\n",
    "    num_train_epochs=5,  # è®­ç»ƒ5ä¸ªepoch(å½“æ•°æ®é‡å°çš„æ—¶å€™è¦é™ä½è½®æ•°é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæœ‰early stopå—)\n",
    "    per_device_train_batch_size=16,  # æ¯ä¸ªGPUè®­ç»ƒ16ä¸ªbatch(ç¡®ä¿æ˜¾å­˜è¶³å¤Ÿ)\n",
    "    per_device_eval_batch_size=32,  # æ¯ä¸ªGPUè¯„ä¼°32ä¸ªbatch(è¯„ä¼°æ—¶æ— éœ€åå‘ä¼ æ’­ï¼Œå¯æ›´å¤§)\n",
    "    gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,     # å¢åŠ warmup(å‰10%stepsçº¿æ€§å¢åŠ lr)\n",
    "    weight_decay=0.01,    # æ·»åŠ æ­£åˆ™åŒ–(é˜²æ­¢è¿‡æ‹Ÿåˆ)\n",
    "    lr_scheduler_type=\"cosine\",  # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡(ä½•æ—¶éœ€è¦ï¼Ÿ)\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,  # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "\n",
    "    save_strategy=\"steps\",  # æŒ‰æ­¥æ•°ä¿å­˜ï¼Œä¸è¯„ä¼°ç­–ç•¥ä¸€è‡´\n",
    "    save_steps=100,  # æ¯100æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    save_total_limit=5,  # è®¾ç½®æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°ï¼Œé¿å…ç£ç›˜çˆ†ç‚¸\n",
    "\n",
    "    load_best_model_at_end=True,  # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"f1\",   # æ ¹æ®F1é€‰æ‹©æœ€ä½³(mrpcä»»åŠ¡æ ¸å¿ƒæŒ‡æ ‡)\n",
    "    greater_is_better=True,  # è¯„ä¼°æŒ‡æ ‡æ˜¯F1ï¼ŒF1è¶Šå¤§è¶Šå¥½\n",
    "\n",
    "    #bf16=torch.cuda.is_bf16_supported(),  # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦æ”¯æŒbf16\n",
    "    #fp16=not torch.cuda.is_bf16_supported(),  # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦æ”¯æŒfp16\n",
    "    dataloader_num_workers=4 if torch.cuda.is_available() else 2,  # å¤šçº¿ç¨‹åŠ è½½æ•°æ®(æŒ‰ç…§CPUæ ¸å¿ƒæ•°åˆ¶å®š)\n",
    "    dataloader_pin_memory=True,  # é”å®šå†…å­˜(åŠ é€Ÿæ•°æ®åŠ è½½)ï¼Œå»ºè®®GPUè®­ç»ƒæ—¶å¼€å¯\n",
    "\n",
    "    logging_dir=f\"{save_path}/{checkpoint}/logs\",  # æ—¥å¿—å•ç‹¬å­˜æ”¾\n",
    "    logging_steps=50,  # æ¯50æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    \n",
    "    report_to=\"tensorboard\",  # å¤šå¹³å°ç›‘æ§\n",
    "    save_safetensors=True,  # å¯ç”¨å®‰å…¨æ ¼å¼\n",
    "\n",
    "    # do_train=True,  # è®­ç»ƒå¼€å…³\n",
    "    # max_steps=15000,  # æ€»è®­ç»ƒæ­¥æ•°\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
